{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy \n",
    "numpy.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, MaxPooling2D, Dropout, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, Embedding\n",
    "from keras.optimizers import SGD, rmsprop\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ELU\n",
    "from keras_contrib.layers.advanced_activations import PELU, SReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "num_classes=1370\n",
    "epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"E:\");\n",
    "#path=\"E:/Dataset_Final(March)/One\";\n",
    "path=\"E:/Dataset_Final(April)/Two\";\n",
    "classes=os.listdir(path)\n",
    "x=[]#Datapoints \n",
    "y=[]#labels \n",
    "for fol in classes:\n",
    "    imgfiles=os.listdir(path+u'\\\\'+fol);\n",
    "    for img in imgfiles:\n",
    "        im=mpimg.imread(path+u'\\\\'+fol+u'\\\\'+img);\n",
    "        x.append(im)\n",
    "        y.append(fol)\n",
    "x=numpy.array(x)\n",
    "y=numpy.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3648, 100, 100)\n",
      "(3648,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "x=x.reshape((-1,100,100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3648\n"
     ]
    }
   ],
   "source": [
    "n=x.shape[0]\n",
    "print (n)\n",
    "randomize=numpy.arange(n)\n",
    "numpy.random.shuffle(randomize)\n",
    "randomize\n",
    "x=x[randomize]\n",
    "y=y[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3283, 100, 100, 1)\n",
      "(3283,)\n",
      "(365, 100, 100, 1)\n",
      "(365,)\n"
     ]
    }
   ],
   "source": [
    "test_split=round(n*2.7/3)\n",
    "x_train=x[:test_split]\n",
    "y_train=y[:test_split]\n",
    "x_test=x[test_split:]\n",
    "y_test=y[test_split:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test=keras.utils.to_categorical(y_test, num_classes)\n",
    "y_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs=Input(shape=(100,100,1))\n",
    "\n",
    "# y=Conv2D(1,(3,3), padding='same')(inputs)\n",
    "# z=keras.layers.add([inputs,y])\n",
    "\n",
    "x=Conv2D(16, (10, 10), padding='same')(inputs)\n",
    "x=SReLU()(x)\n",
    "x=MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "\n",
    "x=Conv2D(32, (9, 9), padding='same')(x)\n",
    "x=SReLU()(x)\n",
    "x=MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "\n",
    "x=Conv2D(64, (8, 8), padding='same')(x)\n",
    "x=SReLU()(x)\n",
    "x=MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "x=Conv2D(128, (3, 3), padding='same')(x)\n",
    "x=SReLU()(x)\n",
    "x=MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "x=Dropout(0.25)(x)\n",
    "x=Flatten()(x)\n",
    "x=Dense(num_classes)(x)\n",
    "output=Activation('softmax')(x)\n",
    "model=Model([inputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 16)      1616      \n",
      "_________________________________________________________________\n",
      "s_re_lu_1 (SReLU)            (None, 100, 100, 16)      640000    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 50, 32)        41504     \n",
      "_________________________________________________________________\n",
      "s_re_lu_2 (SReLU)            (None, 50, 50, 32)        320000    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 25, 64)        131136    \n",
      "_________________________________________________________________\n",
      "s_re_lu_3 (SReLU)            (None, 25, 25, 64)        160000    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "s_re_lu_4 (SReLU)            (None, 12, 12, 128)       73728     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1370)              6314330   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1370)              0         \n",
      "=================================================================\n",
      "Total params: 7,756,170\n",
      "Trainable params: 7,756,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt=keras.optimizers.RMSprop(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath='E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-{epoch:2d}-{val_acc:.2f}.hdf5'\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='val_acc', verbose=1, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3283 samples, validate on 365 samples\n",
      "Epoch 1/25\n",
      "3283/3283 [==============================] - 321s 98ms/step - loss: 7.0800 - acc: 0.0024 - val_loss: 6.4285 - val_acc: 0.0164\n",
      "\n",
      "Epoch 00001: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement- 1-0.02.hdf5\n",
      "Epoch 2/25\n",
      "3283/3283 [==============================] - 367s 112ms/step - loss: 4.4328 - acc: 0.1532 - val_loss: 3.6756 - val_acc: 0.1945\n",
      "\n",
      "Epoch 00002: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement- 2-0.19.hdf5\n",
      "Epoch 3/25\n",
      "3283/3283 [==============================] - 389s 118ms/step - loss: 1.9893 - acc: 0.4596 - val_loss: 2.4481 - val_acc: 0.3753\n",
      "\n",
      "Epoch 00003: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement- 3-0.38.hdf5\n",
      "Epoch 4/25\n",
      "3283/3283 [==============================] - 440s 134ms/step - loss: 1.1431 - acc: 0.6585 - val_loss: 1.5021 - val_acc: 0.6137\n",
      "\n",
      "Epoch 00004: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement- 4-0.61.hdf5\n",
      "Epoch 5/25\n",
      "3283/3283 [==============================] - 407s 124ms/step - loss: 0.7001 - acc: 0.7950 - val_loss: 1.1498 - val_acc: 0.7370\n",
      "\n",
      "Epoch 00005: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement- 5-0.74.hdf5\n",
      "Epoch 6/25\n",
      "3283/3283 [==============================] - 395s 120ms/step - loss: 0.3749 - acc: 0.8836 - val_loss: 0.7525 - val_acc: 0.8219\n",
      "\n",
      "Epoch 00006: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement- 6-0.82.hdf5\n",
      "Epoch 7/25\n",
      "3283/3283 [==============================] - 384s 117ms/step - loss: 0.2212 - acc: 0.9327 - val_loss: 0.7164 - val_acc: 0.8384\n",
      "\n",
      "Epoch 00007: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement- 7-0.84.hdf5\n",
      "Epoch 8/25\n",
      "3283/3283 [==============================] - 342s 104ms/step - loss: 0.1297 - acc: 0.9616 - val_loss: 0.4979 - val_acc: 0.8904\n",
      "\n",
      "Epoch 00008: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement- 8-0.89.hdf5\n",
      "Epoch 9/25\n",
      "3283/3283 [==============================] - 337s 103ms/step - loss: 0.0962 - acc: 0.9750 - val_loss: 0.5122 - val_acc: 0.9068\n",
      "\n",
      "Epoch 00009: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement- 9-0.91.hdf5\n",
      "Epoch 10/25\n",
      "3283/3283 [==============================] - 345s 105ms/step - loss: 0.0553 - acc: 0.9836 - val_loss: 0.4672 - val_acc: 0.9151\n",
      "\n",
      "Epoch 00010: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-10-0.92.hdf5\n",
      "Epoch 11/25\n",
      "3283/3283 [==============================] - 340s 104ms/step - loss: 0.0475 - acc: 0.9860 - val_loss: 0.4330 - val_acc: 0.9342\n",
      "\n",
      "Epoch 00011: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-11-0.93.hdf5\n",
      "Epoch 12/25\n",
      "3283/3283 [==============================] - 341s 104ms/step - loss: 0.0334 - acc: 0.9899 - val_loss: 0.4792 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00012: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-12-0.92.hdf5\n",
      "Epoch 13/25\n",
      "3283/3283 [==============================] - 345s 105ms/step - loss: 0.0245 - acc: 0.9951 - val_loss: 0.3161 - val_acc: 0.9562\n",
      "\n",
      "Epoch 00013: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-13-0.96.hdf5\n",
      "Epoch 14/25\n",
      "3283/3283 [==============================] - 343s 104ms/step - loss: 0.0362 - acc: 0.9924 - val_loss: 0.3518 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00014: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-14-0.95.hdf5\n",
      "Epoch 15/25\n",
      "3283/3283 [==============================] - 363s 111ms/step - loss: 0.0119 - acc: 0.9966 - val_loss: 0.2947 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00015: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-15-0.95.hdf5\n",
      "Epoch 16/25\n",
      "3283/3283 [==============================] - 403s 123ms/step - loss: 0.0229 - acc: 0.9963 - val_loss: 0.3005 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00016: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-16-0.96.hdf5\n",
      "Epoch 17/25\n",
      "3283/3283 [==============================] - 428s 130ms/step - loss: 0.0085 - acc: 0.9985 - val_loss: 0.2717 - val_acc: 0.9616\n",
      "\n",
      "Epoch 00017: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-17-0.96.hdf5\n",
      "Epoch 18/25\n",
      "3283/3283 [==============================] - 444s 135ms/step - loss: 0.0089 - acc: 0.9985 - val_loss: 0.3928 - val_acc: 0.9507\n",
      "\n",
      "Epoch 00018: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-18-0.95.hdf5\n",
      "Epoch 19/25\n",
      "3283/3283 [==============================] - 441s 134ms/step - loss: 0.0137 - acc: 0.9957 - val_loss: 0.3379 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00019: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-19-0.96.hdf5\n",
      "Epoch 20/25\n",
      "3283/3283 [==============================] - 445s 136ms/step - loss: 0.0091 - acc: 0.9982 - val_loss: 0.3504 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00020: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-20-0.95.hdf5\n",
      "Epoch 21/25\n",
      "3283/3283 [==============================] - 440s 134ms/step - loss: 0.0120 - acc: 0.9982 - val_loss: 0.3659 - val_acc: 0.9616\n",
      "\n",
      "Epoch 00021: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-21-0.96.hdf5\n",
      "Epoch 22/25\n",
      "3283/3283 [==============================] - 431s 131ms/step - loss: 0.0104 - acc: 0.9973 - val_loss: 0.3459 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00022: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-22-0.95.hdf5\n",
      "Epoch 23/25\n",
      "3283/3283 [==============================] - 436s 133ms/step - loss: 0.0224 - acc: 0.9982 - val_loss: 0.3337 - val_acc: 0.9616\n",
      "\n",
      "Epoch 00023: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-23-0.96.hdf5\n",
      "Epoch 24/25\n",
      "3283/3283 [==============================] - 416s 127ms/step - loss: 0.0122 - acc: 0.9991 - val_loss: 0.2230 - val_acc: 0.9726\n",
      "\n",
      "Epoch 00024: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-24-0.97.hdf5\n",
      "Epoch 25/25\n",
      "3283/3283 [==============================] - 350s 107ms/step - loss: 0.0134 - acc: 0.9979 - val_loss: 0.3399 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00025: saving model to E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/weights-improvement-25-0.97.hdf5\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(x_train, y_train, \n",
    "               batch_size=50,\n",
    "               epochs=epochs,\n",
    "               validation_data=(x_test, y_test),\n",
    "               callbacks=[checkpoint]\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score=model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)# for dataset of 12 characters\n",
    "print (y_pred)\n",
    "y_pred=numpy.argmax(y_pred, axis=1)\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"E:Dataset_Final(March)/Checkpoints/Level_1/Epoch_41/weights-improvement-14-0.98.hdf5\")# f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "score=model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98039215686274506"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.90199081e-17   2.09929721e-17   1.24922264e-17 ...,   8.88945090e-27\n",
      "    3.39433694e-19   3.19130001e-27]\n",
      " [  4.16776558e-13   6.69391775e-14   1.73623789e-12 ...,   2.32223887e-24\n",
      "    4.06781779e-14   3.80484654e-24]\n",
      " [  7.38239051e-19   2.34063017e-18   6.32201321e-19 ...,   3.94380360e-24\n",
      "    3.25690139e-19   4.92971049e-21]\n",
      " ..., \n",
      " [  1.82910345e-20   1.50606739e-19   1.21145241e-19 ...,   8.85208560e-29\n",
      "    9.35655495e-21   3.05209556e-32]\n",
      " [  4.91960693e-20   2.13653046e-20   4.30499203e-20 ...,   1.54109752e-23\n",
      "    3.21556899e-20   1.41870780e-23]\n",
      " [  1.43401566e-20   9.62631955e-20   7.26449789e-20 ...,   2.05515214e-27\n",
      "    1.25837056e-20   7.37339895e-23]]\n",
      "[1104 1094  867  641  995 1020 1055  935  963  177  351  124  178  305  791\n",
      "  290  364  936  111  948  116  662  249  843  824  713  228  353  285  301\n",
      " 1347  671   93 1115 1080  830 1295  656  859  842  837 1001   91  191 1121\n",
      "  809  131  308  631  219 1096  161   41  657 1294 1010 1042 1285  782  702\n",
      "  694 1131  105  313  228  137  255  929  951  323 1094   68 1267  237  119\n",
      "  970 1261  640 1024  773  963 1049 1269  104  978  980  163  823  231 1185\n",
      " 1028 1059  713  702  187  902 1191 1058  350 1220  274  277 1283  336  169\n",
      "  323  858  307 1190  192  866  795  883  754 1200  872  234 1106  666 1286\n",
      "  674  359  302 1113  639  987  892  370  981  180 1341  142 1138   92  280\n",
      "  197  934  134  641  692  920   66  351 1205  879  657 1268  792  207  828\n",
      "   40 1266 1065  985  307 1366 1044 1068   38  675 1211  903   97 1211 1002\n",
      "  659 1265  921  120  895 1186  733  365 1221  116 1339  866  100  719 1131\n",
      "  208  911 1023 1072  985  993  202 1086  749  944  191 1032  649 1273  350\n",
      "  996  229  986 1106  715  932 1110 1288  206  289  800  248  254   54 1277\n",
      "  930 1362 1018 1335 1013  364  972  769  908  145 1363  243  746 1025  223\n",
      "  145  281  678  308  270  346  681  662  182  825  360 1143 1362   98 1068\n",
      "  345 1351  996  773  105  346  356  369  198  656  233  831  151 1357  203\n",
      "  922  316 1130  878  786 1030  797 1046  659 1120   78  163  658   67 1011\n",
      "  668  301  223  217  990 1061  193  814 1137  724  255  630  362  766  910\n",
      "  758  193 1077  222  945 1083  320  301 1345  136  961  269  260  742  222\n",
      "  321  711  860  225  162  715 1024  699 1110  818 1054 1366 1291   41  921\n",
      "  333 1124  948  744  872  321  206  139  640  741 1272  798 1138  634  941\n",
      "  136 1109 1262  117  154  306  137 1337  901  103  859  288 1083  292   41\n",
      " 1129  650   71  207  232 1058 1074 1365 1286  159  709 1193  651  922 1112\n",
      "  343  920  811  140  745  938  704 1017  136 1338  181  642  670 1350 1062\n",
      "  957  799  126  315 1141  300   59   40  834  369  324  166 1197  973   75\n",
      "  698  705  125  228  316  175   65  964  852  847  822 1271  948 1041  218\n",
      " 1269  173  240  258 1287  208  990  179  697  329  819  887  717  319  267\n",
      "  114   64  332   73 1069  753  260  736 1132 1350  305  924   93  743  970\n",
      " 1353 1066  651 1114  982 1133   99 1281  140  756  884  871  887  797  150\n",
      "  318  285  322 1025  958  652   74 1294  691  154  199 1369 1198   95  321\n",
      "  896  689  342  838 1084 1284 1261  235  897  662 1117 1005 1260  802  319\n",
      " 1278  346  272 1105  296  133]\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)# for dataset of 12 characters\n",
    "print (y_pred)\n",
    "y_pred=numpy.argmax(y_pred, axis=1)\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ..., 0 0 0]\n",
      " [0 2 0 ..., 0 0 0]\n",
      " [0 0 2 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 1 0 0]\n",
      " [0 0 0 ..., 0 2 0]\n",
      " [0 0 0 ..., 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(numpy.argmax(y_test, axis=1),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         38       1.00      1.00      1.00         1\n",
      "         40       1.00      1.00      1.00         2\n",
      "         41       0.67      1.00      0.80         2\n",
      "         42       0.00      0.00      0.00         1\n",
      "         54       1.00      1.00      1.00         1\n",
      "         59       1.00      1.00      1.00         1\n",
      "         64       1.00      1.00      1.00         1\n",
      "         65       1.00      1.00      1.00         1\n",
      "         66       1.00      1.00      1.00         1\n",
      "         67       1.00      0.50      0.67         2\n",
      "         68       1.00      0.50      0.67         2\n",
      "         71       1.00      1.00      1.00         1\n",
      "         73       1.00      1.00      1.00         1\n",
      "         74       1.00      0.50      0.67         2\n",
      "         75       1.00      1.00      1.00         1\n",
      "         78       1.00      1.00      1.00         1\n",
      "         91       1.00      1.00      1.00         1\n",
      "         92       1.00      1.00      1.00         1\n",
      "         93       1.00      1.00      1.00         2\n",
      "         95       1.00      1.00      1.00         1\n",
      "         97       1.00      1.00      1.00         1\n",
      "         98       1.00      1.00      1.00         1\n",
      "         99       1.00      1.00      1.00         1\n",
      "        100       1.00      1.00      1.00         1\n",
      "        103       1.00      1.00      1.00         1\n",
      "        104       1.00      1.00      1.00         1\n",
      "        105       0.50      1.00      0.67         1\n",
      "        111       0.00      0.00      0.00         0\n",
      "        114       1.00      1.00      1.00         1\n",
      "        116       1.00      1.00      1.00         2\n",
      "        117       1.00      1.00      1.00         1\n",
      "        119       1.00      1.00      1.00         1\n",
      "        120       1.00      1.00      1.00         1\n",
      "        124       1.00      1.00      1.00         1\n",
      "        125       1.00      1.00      1.00         1\n",
      "        126       1.00      1.00      1.00         1\n",
      "        131       1.00      1.00      1.00         1\n",
      "        133       1.00      1.00      1.00         1\n",
      "        134       1.00      1.00      1.00         1\n",
      "        136       0.33      1.00      0.50         1\n",
      "        137       1.00      1.00      1.00         2\n",
      "        139       1.00      1.00      1.00         1\n",
      "        140       0.00      0.00      0.00         0\n",
      "        142       1.00      1.00      1.00         1\n",
      "        145       1.00      1.00      1.00         2\n",
      "        150       1.00      1.00      1.00         1\n",
      "        151       1.00      1.00      1.00         1\n",
      "        154       1.00      1.00      1.00         2\n",
      "        159       1.00      1.00      1.00         1\n",
      "        161       1.00      1.00      1.00         1\n",
      "        162       1.00      1.00      1.00         1\n",
      "        163       1.00      1.00      1.00         2\n",
      "        166       1.00      1.00      1.00         1\n",
      "        169       1.00      0.50      0.67         2\n",
      "        173       1.00      1.00      1.00         1\n",
      "        175       1.00      1.00      1.00         1\n",
      "        177       1.00      1.00      1.00         1\n",
      "        178       0.00      0.00      0.00         0\n",
      "        179       1.00      1.00      1.00         1\n",
      "        180       1.00      1.00      1.00         1\n",
      "        181       1.00      1.00      1.00         1\n",
      "        182       1.00      1.00      1.00         1\n",
      "        187       1.00      1.00      1.00         1\n",
      "        191       0.50      1.00      0.67         1\n",
      "        192       1.00      1.00      1.00         1\n",
      "        193       1.00      0.67      0.80         3\n",
      "        197       1.00      1.00      1.00         1\n",
      "        198       1.00      1.00      1.00         1\n",
      "        199       1.00      1.00      1.00         1\n",
      "        202       1.00      1.00      1.00         1\n",
      "        203       1.00      1.00      1.00         1\n",
      "        206       0.50      1.00      0.67         1\n",
      "        207       1.00      1.00      1.00         2\n",
      "        208       1.00      1.00      1.00         2\n",
      "        215       0.00      0.00      0.00         1\n",
      "        217       1.00      1.00      1.00         1\n",
      "        218       1.00      1.00      1.00         1\n",
      "        219       1.00      1.00      1.00         1\n",
      "        222       1.00      1.00      1.00         2\n",
      "        223       1.00      1.00      1.00         2\n",
      "        225       1.00      1.00      1.00         1\n",
      "        227       0.00      0.00      0.00         1\n",
      "        228       0.67      1.00      0.80         2\n",
      "        229       1.00      1.00      1.00         1\n",
      "        231       1.00      1.00      1.00         1\n",
      "        232       1.00      1.00      1.00         1\n",
      "        233       1.00      1.00      1.00         1\n",
      "        234       1.00      1.00      1.00         1\n",
      "        235       1.00      1.00      1.00         1\n",
      "        237       1.00      1.00      1.00         1\n",
      "        240       1.00      1.00      1.00         1\n",
      "        243       1.00      1.00      1.00         1\n",
      "        244       0.00      0.00      0.00         1\n",
      "        248       1.00      1.00      1.00         1\n",
      "        249       1.00      1.00      1.00         1\n",
      "        254       1.00      1.00      1.00         1\n",
      "        255       1.00      1.00      1.00         2\n",
      "        258       1.00      1.00      1.00         1\n",
      "        260       1.00      1.00      1.00         2\n",
      "        267       1.00      1.00      1.00         1\n",
      "        269       1.00      1.00      1.00         1\n",
      "        270       1.00      1.00      1.00         1\n",
      "        272       1.00      1.00      1.00         1\n",
      "        274       1.00      1.00      1.00         1\n",
      "        277       1.00      1.00      1.00         1\n",
      "        280       1.00      1.00      1.00         1\n",
      "        281       0.00      0.00      0.00         0\n",
      "        285       1.00      1.00      1.00         2\n",
      "        288       1.00      1.00      1.00         1\n",
      "        289       1.00      1.00      1.00         1\n",
      "        290       1.00      1.00      1.00         1\n",
      "        292       1.00      1.00      1.00         1\n",
      "        296       1.00      1.00      1.00         1\n",
      "        300       1.00      1.00      1.00         1\n",
      "        301       1.00      1.00      1.00         3\n",
      "        302       1.00      1.00      1.00         1\n",
      "        305       1.00      1.00      1.00         2\n",
      "        306       1.00      1.00      1.00         1\n",
      "        307       1.00      1.00      1.00         2\n",
      "        308       1.00      1.00      1.00         2\n",
      "        313       1.00      1.00      1.00         1\n",
      "        315       1.00      1.00      1.00         1\n",
      "        316       0.50      1.00      0.67         1\n",
      "        318       1.00      1.00      1.00         1\n",
      "        319       1.00      1.00      1.00         2\n",
      "        320       1.00      1.00      1.00         1\n",
      "        321       0.33      1.00      0.50         1\n",
      "        322       1.00      1.00      1.00         1\n",
      "        323       1.00      1.00      1.00         2\n",
      "        324       1.00      1.00      1.00         1\n",
      "        329       0.00      0.00      0.00         0\n",
      "        332       1.00      1.00      1.00         1\n",
      "        333       1.00      1.00      1.00         1\n",
      "        336       1.00      1.00      1.00         1\n",
      "        342       1.00      1.00      1.00         1\n",
      "        343       1.00      1.00      1.00         1\n",
      "        345       1.00      1.00      1.00         1\n",
      "        346       0.33      1.00      0.50         1\n",
      "        350       1.00      1.00      1.00         2\n",
      "        351       1.00      1.00      1.00         2\n",
      "        353       1.00      1.00      1.00         1\n",
      "        356       1.00      1.00      1.00         1\n",
      "        358       0.00      0.00      0.00         3\n",
      "        359       1.00      1.00      1.00         1\n",
      "        360       1.00      1.00      1.00         1\n",
      "        362       1.00      1.00      1.00         1\n",
      "        364       1.00      1.00      1.00         2\n",
      "        365       1.00      1.00      1.00         1\n",
      "        366       0.00      0.00      0.00         4\n",
      "        369       1.00      1.00      1.00         2\n",
      "        370       1.00      1.00      1.00         1\n",
      "        630       1.00      1.00      1.00         1\n",
      "        631       1.00      1.00      1.00         1\n",
      "        634       1.00      1.00      1.00         1\n",
      "        639       1.00      1.00      1.00         1\n",
      "        640       1.00      1.00      1.00         2\n",
      "        641       1.00      1.00      1.00         2\n",
      "        642       1.00      1.00      1.00         1\n",
      "        649       1.00      1.00      1.00         1\n",
      "        650       1.00      1.00      1.00         1\n",
      "        651       1.00      1.00      1.00         2\n",
      "        652       1.00      1.00      1.00         1\n",
      "        656       1.00      1.00      1.00         2\n",
      "        657       1.00      1.00      1.00         2\n",
      "        658       1.00      1.00      1.00         1\n",
      "        659       1.00      1.00      1.00         2\n",
      "        662       1.00      1.00      1.00         3\n",
      "        666       1.00      1.00      1.00         1\n",
      "        668       1.00      1.00      1.00         1\n",
      "        670       1.00      1.00      1.00         1\n",
      "        671       1.00      1.00      1.00         1\n",
      "        674       1.00      1.00      1.00         1\n",
      "        675       1.00      1.00      1.00         1\n",
      "        678       0.00      0.00      0.00         0\n",
      "        681       1.00      1.00      1.00         1\n",
      "        689       1.00      1.00      1.00         1\n",
      "        691       0.00      0.00      0.00         0\n",
      "        692       1.00      1.00      1.00         1\n",
      "        694       1.00      1.00      1.00         1\n",
      "        697       1.00      1.00      1.00         1\n",
      "        698       1.00      1.00      1.00         1\n",
      "        699       1.00      1.00      1.00         1\n",
      "        702       1.00      1.00      1.00         2\n",
      "        704       1.00      1.00      1.00         1\n",
      "        705       1.00      1.00      1.00         1\n",
      "        709       1.00      1.00      1.00         1\n",
      "        711       1.00      1.00      1.00         1\n",
      "        713       1.00      1.00      1.00         2\n",
      "        715       0.50      1.00      0.67         1\n",
      "        717       1.00      1.00      1.00         1\n",
      "        719       1.00      1.00      1.00         1\n",
      "        724       1.00      1.00      1.00         1\n",
      "        733       1.00      1.00      1.00         1\n",
      "        736       1.00      1.00      1.00         1\n",
      "        741       1.00      1.00      1.00         1\n",
      "        742       1.00      1.00      1.00         1\n",
      "        743       1.00      1.00      1.00         1\n",
      "        744       1.00      1.00      1.00         1\n",
      "        745       1.00      1.00      1.00         1\n",
      "        746       1.00      1.00      1.00         1\n",
      "        749       1.00      1.00      1.00         1\n",
      "        752       0.00      0.00      0.00         2\n",
      "        753       1.00      1.00      1.00         1\n",
      "        754       1.00      1.00      1.00         1\n",
      "        756       1.00      1.00      1.00         1\n",
      "        758       1.00      1.00      1.00         1\n",
      "        766       1.00      1.00      1.00         1\n",
      "        769       1.00      1.00      1.00         1\n",
      "        773       1.00      1.00      1.00         2\n",
      "        782       1.00      1.00      1.00         1\n",
      "        786       1.00      1.00      1.00         1\n",
      "        791       1.00      1.00      1.00         1\n",
      "        792       1.00      1.00      1.00         1\n",
      "        795       1.00      1.00      1.00         1\n",
      "        797       1.00      1.00      1.00         2\n",
      "        798       1.00      1.00      1.00         1\n",
      "        799       1.00      1.00      1.00         1\n",
      "        800       1.00      1.00      1.00         1\n",
      "        802       1.00      1.00      1.00         1\n",
      "        809       1.00      1.00      1.00         1\n",
      "        811       1.00      1.00      1.00         1\n",
      "        814       1.00      1.00      1.00         1\n",
      "        818       1.00      1.00      1.00         1\n",
      "        819       1.00      1.00      1.00         1\n",
      "        822       1.00      1.00      1.00         1\n",
      "        823       1.00      1.00      1.00         1\n",
      "        824       1.00      1.00      1.00         1\n",
      "        825       1.00      1.00      1.00         1\n",
      "        828       1.00      0.33      0.50         3\n",
      "        830       0.00      0.00      0.00         0\n",
      "        831       1.00      1.00      1.00         1\n",
      "        834       1.00      1.00      1.00         1\n",
      "        837       1.00      1.00      1.00         1\n",
      "        838       1.00      1.00      1.00         1\n",
      "        842       1.00      1.00      1.00         1\n",
      "        843       1.00      1.00      1.00         1\n",
      "        847       0.00      0.00      0.00         0\n",
      "        852       0.00      0.00      0.00         0\n",
      "        858       1.00      1.00      1.00         1\n",
      "        859       1.00      1.00      1.00         2\n",
      "        860       1.00      1.00      1.00         1\n",
      "        866       1.00      1.00      1.00         2\n",
      "        867       0.00      0.00      0.00         0\n",
      "        871       1.00      1.00      1.00         1\n",
      "        872       1.00      0.67      0.80         3\n",
      "        878       1.00      1.00      1.00         1\n",
      "        879       1.00      1.00      1.00         1\n",
      "        883       1.00      1.00      1.00         1\n",
      "        884       1.00      1.00      1.00         1\n",
      "        887       1.00      1.00      1.00         2\n",
      "        889       0.00      0.00      0.00         1\n",
      "        892       1.00      1.00      1.00         1\n",
      "        895       1.00      1.00      1.00         1\n",
      "        896       1.00      1.00      1.00         1\n",
      "        897       1.00      1.00      1.00         1\n",
      "        901       1.00      1.00      1.00         1\n",
      "        902       1.00      1.00      1.00         1\n",
      "        903       1.00      1.00      1.00         1\n",
      "        908       1.00      1.00      1.00         1\n",
      "        910       1.00      1.00      1.00         1\n",
      "        911       1.00      0.33      0.50         3\n",
      "        914       0.00      0.00      0.00         1\n",
      "        920       1.00      1.00      1.00         2\n",
      "        921       0.50      1.00      0.67         1\n",
      "        922       1.00      1.00      1.00         2\n",
      "        924       1.00      1.00      1.00         1\n",
      "        926       0.00      0.00      0.00         1\n",
      "        929       1.00      1.00      1.00         1\n",
      "        930       1.00      1.00      1.00         1\n",
      "        932       1.00      1.00      1.00         1\n",
      "        934       1.00      1.00      1.00         1\n",
      "        935       1.00      1.00      1.00         1\n",
      "        936       1.00      1.00      1.00         1\n",
      "        938       1.00      1.00      1.00         1\n",
      "        941       1.00      1.00      1.00         1\n",
      "        944       1.00      1.00      1.00         1\n",
      "        945       1.00      1.00      1.00         1\n",
      "        948       0.33      1.00      0.50         1\n",
      "        951       0.00      0.00      0.00         0\n",
      "        957       1.00      1.00      1.00         1\n",
      "        958       1.00      1.00      1.00         1\n",
      "        961       1.00      1.00      1.00         1\n",
      "        963       0.50      1.00      0.67         1\n",
      "        964       1.00      1.00      1.00         1\n",
      "        970       1.00      1.00      1.00         2\n",
      "        972       1.00      1.00      1.00         1\n",
      "        973       1.00      1.00      1.00         1\n",
      "        978       1.00      1.00      1.00         1\n",
      "        980       1.00      1.00      1.00         1\n",
      "        981       1.00      1.00      1.00         1\n",
      "        982       1.00      1.00      1.00         1\n",
      "        985       1.00      1.00      1.00         2\n",
      "        986       1.00      1.00      1.00         1\n",
      "        987       1.00      1.00      1.00         1\n",
      "        990       1.00      1.00      1.00         2\n",
      "        993       1.00      1.00      1.00         1\n",
      "        995       1.00      1.00      1.00         1\n",
      "        996       1.00      1.00      1.00         2\n",
      "       1001       1.00      1.00      1.00         1\n",
      "       1002       1.00      1.00      1.00         1\n",
      "       1005       1.00      1.00      1.00         1\n",
      "       1010       1.00      1.00      1.00         1\n",
      "       1011       1.00      1.00      1.00         1\n",
      "       1013       1.00      1.00      1.00         1\n",
      "       1017       1.00      1.00      1.00         1\n",
      "       1018       1.00      1.00      1.00         1\n",
      "       1020       1.00      1.00      1.00         1\n",
      "       1023       1.00      1.00      1.00         1\n",
      "       1024       1.00      1.00      1.00         2\n",
      "       1025       1.00      1.00      1.00         2\n",
      "       1028       1.00      1.00      1.00         1\n",
      "       1030       1.00      1.00      1.00         1\n",
      "       1032       1.00      1.00      1.00         1\n",
      "       1041       1.00      1.00      1.00         1\n",
      "       1042       1.00      1.00      1.00         1\n",
      "       1044       1.00      1.00      1.00         1\n",
      "       1046       1.00      1.00      1.00         1\n",
      "       1049       1.00      1.00      1.00         1\n",
      "       1054       1.00      1.00      1.00         1\n",
      "       1055       1.00      1.00      1.00         1\n",
      "       1058       1.00      1.00      1.00         2\n",
      "       1059       1.00      1.00      1.00         1\n",
      "       1061       1.00      1.00      1.00         1\n",
      "       1062       1.00      1.00      1.00         1\n",
      "       1065       1.00      1.00      1.00         1\n",
      "       1066       1.00      1.00      1.00         1\n",
      "       1068       1.00      1.00      1.00         2\n",
      "       1069       1.00      1.00      1.00         1\n",
      "       1072       1.00      1.00      1.00         1\n",
      "       1074       1.00      1.00      1.00         1\n",
      "       1077       1.00      1.00      1.00         1\n",
      "       1080       1.00      1.00      1.00         1\n",
      "       1083       0.00      0.00      0.00         0\n",
      "       1084       1.00      1.00      1.00         1\n",
      "       1086       1.00      1.00      1.00         1\n",
      "       1094       1.00      1.00      1.00         2\n",
      "       1096       1.00      1.00      1.00         1\n",
      "       1104       1.00      1.00      1.00         1\n",
      "       1105       1.00      1.00      1.00         1\n",
      "       1106       1.00      1.00      1.00         2\n",
      "       1109       1.00      1.00      1.00         1\n",
      "       1110       1.00      1.00      1.00         2\n",
      "       1112       1.00      1.00      1.00         1\n",
      "       1113       1.00      1.00      1.00         1\n",
      "       1114       1.00      1.00      1.00         1\n",
      "       1115       1.00      1.00      1.00         1\n",
      "       1117       1.00      1.00      1.00         1\n",
      "       1120       1.00      1.00      1.00         1\n",
      "       1121       1.00      1.00      1.00         1\n",
      "       1124       1.00      1.00      1.00         1\n",
      "       1126       0.00      0.00      0.00         1\n",
      "       1129       1.00      1.00      1.00         1\n",
      "       1130       1.00      1.00      1.00         1\n",
      "       1131       1.00      1.00      1.00         2\n",
      "       1132       1.00      1.00      1.00         1\n",
      "       1133       1.00      1.00      1.00         1\n",
      "       1137       1.00      1.00      1.00         1\n",
      "       1138       1.00      1.00      1.00         2\n",
      "       1141       1.00      1.00      1.00         1\n",
      "       1143       1.00      1.00      1.00         1\n",
      "       1185       1.00      1.00      1.00         1\n",
      "       1186       1.00      1.00      1.00         1\n",
      "       1190       1.00      1.00      1.00         1\n",
      "       1191       1.00      1.00      1.00         1\n",
      "       1193       1.00      1.00      1.00         1\n",
      "       1194       0.00      0.00      0.00         2\n",
      "       1197       1.00      1.00      1.00         1\n",
      "       1198       1.00      1.00      1.00         1\n",
      "       1200       1.00      1.00      1.00         1\n",
      "       1205       1.00      1.00      1.00         1\n",
      "       1210       0.00      0.00      0.00         2\n",
      "       1211       1.00      1.00      1.00         2\n",
      "       1214       0.00      0.00      0.00         2\n",
      "       1220       1.00      1.00      1.00         1\n",
      "       1221       1.00      1.00      1.00         1\n",
      "       1260       1.00      1.00      1.00         1\n",
      "       1261       1.00      1.00      1.00         2\n",
      "       1262       1.00      1.00      1.00         1\n",
      "       1265       1.00      1.00      1.00         1\n",
      "       1266       1.00      1.00      1.00         1\n",
      "       1267       1.00      1.00      1.00         1\n",
      "       1268       1.00      1.00      1.00         1\n",
      "       1269       1.00      1.00      1.00         2\n",
      "       1271       1.00      1.00      1.00         1\n",
      "       1272       1.00      1.00      1.00         1\n",
      "       1273       1.00      1.00      1.00         1\n",
      "       1277       1.00      1.00      1.00         1\n",
      "       1278       1.00      1.00      1.00         1\n",
      "       1281       1.00      1.00      1.00         1\n",
      "       1283       1.00      1.00      1.00         1\n",
      "       1284       1.00      1.00      1.00         1\n",
      "       1285       1.00      1.00      1.00         1\n",
      "       1286       1.00      1.00      1.00         2\n",
      "       1287       1.00      1.00      1.00         1\n",
      "       1288       1.00      1.00      1.00         1\n",
      "       1291       1.00      1.00      1.00         1\n",
      "       1294       1.00      1.00      1.00         2\n",
      "       1295       1.00      1.00      1.00         1\n",
      "       1335       1.00      1.00      1.00         1\n",
      "       1337       1.00      1.00      1.00         1\n",
      "       1338       1.00      1.00      1.00         1\n",
      "       1339       1.00      1.00      1.00         1\n",
      "       1341       1.00      1.00      1.00         1\n",
      "       1345       1.00      1.00      1.00         1\n",
      "       1347       1.00      1.00      1.00         1\n",
      "       1350       1.00      1.00      1.00         2\n",
      "       1351       1.00      1.00      1.00         1\n",
      "       1353       1.00      1.00      1.00         1\n",
      "       1357       1.00      1.00      1.00         1\n",
      "       1362       1.00      1.00      1.00         2\n",
      "       1363       0.00      0.00      0.00         0\n",
      "       1365       1.00      1.00      1.00         1\n",
      "       1366       1.00      1.00      1.00         2\n",
      "       1369       1.00      1.00      1.00         1\n",
      "\n",
      "avg / total       0.94      0.93      0.93       486\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (classification_report(numpy.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "pandas.DataFrame(hist.history).to_csv(\"E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/Figure/History-970.csv\")        \n",
    "# visualizing losses and accuracy\n",
    "train_loss=hist.history['loss']\n",
    "val_loss=hist.history['val_loss']\n",
    "train_acc=hist.history['acc']\n",
    "val_acc=hist.history['val_acc']\n",
    "xc=range(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark-palette', 'seaborn-dark', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'seaborn', 'Solarize_Light2', '_classic_test']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "pp=PdfPages(\"E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/Figure/Loss-970.pdf\")\n",
    "plt.figure(1,figsize=(7,5))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,val_loss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('train_loss vs val_loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','val'])\n",
    "print (plt.style.available)\n",
    "plt.style.use(['classic'])\n",
    "plt.savefig(pp, format='pdf')\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp=PdfPages(\"E:Dataset_Final(March)/Checkpoints/Level_2/Epoch_41/Figure/Accuracy-970.pdf\")\n",
    "plt.figure(2,figsize=(7,5))\n",
    "plt.plot(xc,train_acc)\n",
    "plt.plot(xc,val_acc)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('train_acc vs val_acc')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','val'],loc=4)\n",
    "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "plt.style.use(['bmh'])\n",
    "plt.savefig(pp, format='pdf')\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
